---
title: "Gaussian Graphical Model on a 4-Cycle"
output:
  html_document: default
  pdf_document: default
---

# Introduction

In this notebook we study a 4-dimensional Gaussian graphical model whose 
precision matrix corresponds to the **cycle graph**:

\[
1 \;-\; 2 \;-\; 3 \;-\; 4 \;-\; 1.
\]

Zeros in the **precision matrix** encode conditional independences.  
We will:

1. Define the precision matrix \( \Theta \)
2. Compute the covariance matrix \( \Sigma = \Theta^{-1} \)
3. Simulate a sample \(X_1,\dots,X_N \sim \mathcal{N}(0, \Sigma)\)
4. Standardize the margins to variance 1
5. Estimate the precision matrix as the inverse of the MLE covariance matrix
6. Estimate the precision matrix using the MLE estimator
7. Compare the two estimates

---

# 1. Precision Matrix and Covariance

Consider the following precision matrix 

$$
\Theta =
\begin{pmatrix}
10 & 5 & 0 & 3 \\
5 & 10 & 5 & 0 \\
0 & 5 & 10 & 5 \\
3 & 0 & 5 & 10
\end{pmatrix}.
$$
We define the matrix $\Sigma$ as the inverse of $\Theta$, that is, $\Sigma = \Theta^{-1}$.
```{r}
library(MASS)

# 4-cycle precision matrix
Theta <- matrix(c(
  10,  5,  0,  3,
   5, 10,  5,  0,
   0,  5, 10,  5,
   3,  0,  5, 10
), nrow = 4, byrow = TRUE)

# Covariance matrix = inverse of precision
Sigma <- solve(Theta)
Sigma
```

---

# 2. Simulate a Gaussian Sample

We simulate a large sample from a centered multivariate normal distribution with covariance matrix $\Sigma$.

```{r}
N <- 10^6
d <- 4
mean_vec <- rep(0, d)

set.seed(1)
X <- mvrnorm(N, mu = mean_vec, Sigma = Sigma)
head(X)
```

---

# 3. Standardize Margins to Variance 1
We standardize the margins to variance 1
```{r}
std <- sqrt(diag(Sigma))
X_std <- sweep(X, 2, std, FUN = "/")

# Verify variances are ~1
apply(X_std, 2, var)
```

---

# 4. Estimate Covariance and Precision From the Data
We compute the MLE covariance matrix $$
S = \frac{1}{n} \sum_{i =1}^n X_i^t X_i
$$
from our sample and then invert it to get an estimate of the precision matrix. 

```{r}
EstimSigma <- cov(X_std)
MLE_EstimSigma = ((N-1)/N) * EstimSigma
EstimTheta <- solve(EstimSigma)


cat("This is the MLE for covariance matrix computed from the sample:\n")
print(MLE_EstimSigma)
cat("This is the estimate of the precision matrix computed as the inverse of the MLE covariance matrix:\n")
print(EstimTheta)

```

---

# 5. Maximum-Likelihood Estimation of Θ

We parameterize Θ using only the **upper triangular entries** and then define the log-likelihood function given by 
$$
L(\Theta) =  \log \det(\Theta) - trace (S \cdot \Theta) 
$$
where $S$ is MLE covariance matrix, that is:
$$
S = \frac{1}{n} \sum_{i =1}^n X_i^t X_i
$$


### Helper Functions

```{r}
# Construct symmetric matrix from upper-triangular vector
construct_symmetric_matrix <- function(theta_vec, d) {
  Theta_mat <- matrix(0, d, d)
  idx <- which(upper.tri(Theta_mat, diag = TRUE))
  Theta_mat[idx] <- theta_vec
  Theta_mat <- Theta_mat + t(Theta_mat) - diag(diag(Theta_mat))
  return(Theta_mat)
}

# Gaussian log-likelihood
log_likelihood <- function(theta_vec, S, d) {
  Theta_mat <- construct_symmetric_matrix(theta_vec, d)
  det_val <- det(Theta_mat)
  if (det_val <= 0) return(-10^(16))
  return( log(det_val) - sum(diag  (S %*% Theta_mat )) )
}

```

---

# 

---

# Optimization

We minimize the negative loglikelihood function using the function **optim()** with the **Broyden–Fletcher–Goldfarb–Shanno algorithm** method and we randomly choose initial values for the first step of the minimization.

```{r}
#Initial Positive-Definite Guess for the minimization
set.seed(7)
A <- matrix(rnorm(d*d), d, d)
Theta0 <- A %*% t(A) + d * diag(d)

upper_idx <- which(upper.tri(Theta0, diag = TRUE))
theta0_vec <- Theta0[upper_idx]

optim_result <- optim(
  par = theta0_vec,
  fn = function(par) -log_likelihood(par, EstimSigma, d),
  method = "BFGS"
)

Theta_MLE <- construct_symmetric_matrix(optim_result$par, d)
cat("Estimated Precision Matrix (Theta) using implemented LASSO log-likelihood function:\n")
print(Theta_MLE)
```

---

# Conclusion


-We simulated a Gaussian vector consistent with a given graph structure.

-The precision matrix was initially estimated by inverting the MLE for the covariance matrix.

-We then computed the MLE of the precision matrix by maximizing the log-likelihood (equivalently, minimizing the negative log-likelihood).

-Both methods yielded the same results, confirming that the MLE for the precision matrix coincides with the inverse of the sample covariance matrix.

-However, neither method identified exact zeros for $\theta_{13}$ and $\theta_{24}$
. Introducing a lasso penalization with an appropriate regularization term could address this limitation.
